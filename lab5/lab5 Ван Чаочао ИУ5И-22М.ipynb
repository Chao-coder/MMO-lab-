{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e892e4cc",
   "metadata": {},
   "source": [
    "# Лабораторная работа  5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45803f4",
   "metadata": {},
   "source": [
    "# Предобработка и классификация текстовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9246cbb9",
   "metadata": {},
   "source": [
    "Цель лабораторной работы: изучение методов предобработки и классификации текстовых данных."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde419a5",
   "metadata": {},
   "source": [
    "# Требования к отчету:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18216edf",
   "metadata": {},
   "source": [
    "Отчет по лабораторной работе должен содержать:\n",
    "\n",
    "титульный лист;\n",
    "описание задания;\n",
    "текст программы;\n",
    "экранные формы с примерами выполнения программы."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e75809",
   "metadata": {},
   "source": [
    "# Задание:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3f029ec",
   "metadata": {},
   "source": [
    "Для произвольного предложения или текста решите следующие задачи:\n",
    "\n",
    "Токенизация.\n",
    "\n",
    "Частеречная разметка.\n",
    "\n",
    "Лемматизация.\n",
    "\n",
    "Выделение (распознавание) именованных сущностей.\n",
    "\n",
    "Разбор предложения.\n",
    "\n",
    "Для произвольного набора данных, предназначенного для классификации \n",
    "\n",
    "текстов, решите задачу классификации текста двумя способами:\n",
    "\n",
    "Способ 1. На основе CountVectorizer или TfidfVectorizer.\n",
    "\n",
    "Способ 2. На основе моделей word2vec или Glove или fastText.\n",
    "\n",
    "Сравните качество полученных моделей.\n",
    "\n",
    "Для поиска наборов данных в поисковой системе можно использовать ключевые слова \"datasets for text classification\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "08fac2a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting natasha\n",
      "  Downloading natasha-1.4.0-py3-none-any.whl (34.4 MB)\n",
      "Collecting pymorphy2\n",
      "  Downloading pymorphy2-0.9.1-py3-none-any.whl (55 kB)\n",
      "Collecting slovnet>=0.3.0"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading slovnet-0.5.0-py3-none-any.whl (49 kB)\n",
      "Collecting razdel>=0.5.0\n",
      "  Downloading razdel-0.5.0-py3-none-any.whl (21 kB)\n",
      "Collecting yargy>=0.14.0\n",
      "  Downloading yargy-0.15.0-py3-none-any.whl (41 kB)\n",
      "Collecting navec>=0.9.0\n",
      "  Downloading navec-0.10.0-py3-none-any.whl (23 kB)\n",
      "Collecting ipymarkup>=0.8.0\n",
      "  Downloading ipymarkup-0.9.0-py3-none-any.whl (14 kB)\n",
      "Requirement already satisfied: intervaltree>=3 in c:\\programdata\\anaconda3\\lib\\site-packages (from ipymarkup>=0.8.0->natasha) (3.1.0)\n",
      "Requirement already satisfied: sortedcontainers<3.0,>=2.0 in c:\\programdata\\anaconda3\\lib\\site-packages (from intervaltree>=3->ipymarkup>=0.8.0->natasha) (2.4.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from navec>=0.9.0->natasha) (1.22.3)\n",
      "Collecting pymorphy2-dicts-ru<3.0,>=2.4\n",
      "  Downloading pymorphy2_dicts_ru-2.4.417127.4579844-py2.py3-none-any.whl (8.2 MB)\n",
      "Collecting dawg-python>=0.7.1\n",
      "  Downloading DAWG_Python-0.7.2-py2.py3-none-any.whl (11 kB)\n",
      "Collecting docopt>=0.6\n",
      "  Downloading docopt-0.6.2.tar.gz (25 kB)\n",
      "Building wheels for collected packages: docopt\n",
      "  Building wheel for docopt (setup.py): started\n",
      "  Building wheel for docopt (setup.py): finished with status 'done'\n",
      "  Created wheel for docopt: filename=docopt-0.6.2-py2.py3-none-any.whl size=13724 sha256=c2e2a10f3a935820cfcf485828805f3b4a953343d7838cb10a6f365936fd5e4b\n",
      "  Stored in directory: c:\\users\\asus\\appdata\\local\\pip\\cache\\wheels\\70\\4a\\46\\1309fc853b8d395e60bafaf1b6df7845bdd82c95fd59dd8d2b\n",
      "Successfully built docopt\n",
      "Installing collected packages: pymorphy2-dicts-ru, docopt, dawg-python, razdel, pymorphy2, navec, yargy, slovnet, ipymarkup, natasha\n",
      "Successfully installed dawg-python-0.7.2 docopt-0.6.2 ipymarkup-0.9.0 natasha-1.4.0 navec-0.10.0 pymorphy2-0.9.1 pymorphy2-dicts-ru-2.4.417127.4579844 razdel-0.5.0 slovnet-0.5.0 yargy-0.15.0\n"
     ]
    }
   ],
   "source": [
    "!pip install natasha"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32642b48",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "0d4e9565",
   "metadata": {},
   "outputs": [],
   "source": [
    "text1 ='Моско́вский госуда́рственный техни́ческий университе́т им. Н. Э. Ба́умана[a] (также известен как Ба́уманка, Ба́уманский, МГТУ, МВТУ) — российский национальный исследовательский университет, научный центр, особо ценный объект культурного наследия народов России'\n",
    "text2 ='杭州因风景秀丽，素有“人间天堂”的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0200b396",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Моско́вский госуда́рственный техни́ческий университе́т им. Н. Э. Ба́умана[a] (также известен как Ба́уманка, Ба́уманский, МГТУ, МВТУ) — российский национальный исследовательский университет, научный центр, особо ценный объект культурного наследия народов России'"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "59cb6c97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from razdel import tokenize, sentenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c9a23507",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0, 11, 'Моско́вский'),\n",
       " Substring(12, 28, 'госуда́рственный'),\n",
       " Substring(29, 41, 'техни́ческий'),\n",
       " Substring(42, 54, 'университе́т'),\n",
       " Substring(55, 57, 'им'),\n",
       " Substring(57, 58, '.'),\n",
       " Substring(59, 60, 'Н'),\n",
       " Substring(60, 61, '.'),\n",
       " Substring(62, 63, 'Э'),\n",
       " Substring(63, 64, '.'),\n",
       " Substring(65, 73, 'Ба́умана'),\n",
       " Substring(73, 74, '['),\n",
       " Substring(74, 75, 'a'),\n",
       " Substring(75, 76, ']'),\n",
       " Substring(77, 78, '('),\n",
       " Substring(78, 83, 'также'),\n",
       " Substring(84, 92, 'известен'),\n",
       " Substring(93, 96, 'как'),\n",
       " Substring(97, 106, 'Ба́уманка'),\n",
       " Substring(106, 107, ','),\n",
       " Substring(108, 119, 'Ба́уманский'),\n",
       " Substring(119, 120, ','),\n",
       " Substring(121, 125, 'МГТУ'),\n",
       " Substring(125, 126, ','),\n",
       " Substring(127, 131, 'МВТУ'),\n",
       " Substring(131, 132, ')'),\n",
       " Substring(133, 134, '—'),\n",
       " Substring(135, 145, 'российский'),\n",
       " Substring(146, 158, 'национальный'),\n",
       " Substring(159, 176, 'исследовательский'),\n",
       " Substring(177, 188, 'университет'),\n",
       " Substring(188, 189, ','),\n",
       " Substring(190, 197, 'научный'),\n",
       " Substring(198, 203, 'центр'),\n",
       " Substring(203, 204, ','),\n",
       " Substring(205, 210, 'особо'),\n",
       " Substring(211, 217, 'ценный'),\n",
       " Substring(218, 224, 'объект'),\n",
       " Substring(225, 236, 'культурного'),\n",
       " Substring(237, 245, 'наследия'),\n",
       " Substring(246, 253, 'народов'),\n",
       " Substring(254, 260, 'России')]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_tok_text = list(tokenize(text1))\n",
    "n_tok_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "ab3259f6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Substring(0,\n",
       "           260,\n",
       "           'Моско́вский госуда́рственный техни́ческий университе́т им. Н. Э. Ба́умана[a] (также известен как Ба́уманка, Ба́уманский, МГТУ, МВТУ) — российский национальный исследовательский университет, научный центр, особо ценный объект культурного наследия народов России')]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_text = list(sentenize(text1))\n",
    "n_sen_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "25271942",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Моско́вский госуда́рственный техни́ческий университе́т им. Н. Э. Ба́умана[a] (также известен как Ба́уманка, Ба́уманский, МГТУ, МВТУ) — российский национальный исследовательский университет, научный центр, особо ценный объект культурного наследия народов России'],\n",
       " 1)"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[_.text for _ in n_sen_text], len([_.text for _ in n_sen_text])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2216e31b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Этот вариант токенизации нужен для последующей обработки\n",
    "def n_sentenize(text):\n",
    "    n_sen_chunk = []\n",
    "    for sent in sentenize(text):\n",
    "        tokens = [_.text for _ in tokenize(sent.text)]\n",
    "        n_sen_chunk.append(tokens)\n",
    "    return n_sen_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "0c1d9766",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['Моско́вский',\n",
       "  'госуда́рственный',\n",
       "  'техни́ческий',\n",
       "  'университе́т',\n",
       "  'им',\n",
       "  '.',\n",
       "  'Н',\n",
       "  '.',\n",
       "  'Э',\n",
       "  '.',\n",
       "  'Ба́умана',\n",
       "  '[',\n",
       "  'a',\n",
       "  ']',\n",
       "  '(',\n",
       "  'также',\n",
       "  'известен',\n",
       "  'как',\n",
       "  'Ба́уманка',\n",
       "  ',',\n",
       "  'Ба́уманский',\n",
       "  ',',\n",
       "  'МГТУ',\n",
       "  ',',\n",
       "  'МВТУ',\n",
       "  ')',\n",
       "  '—',\n",
       "  'российский',\n",
       "  'национальный',\n",
       "  'исследовательский',\n",
       "  'университет',\n",
       "  ',',\n",
       "  'научный',\n",
       "  'центр',\n",
       "  ',',\n",
       "  'особо',\n",
       "  'ценный',\n",
       "  'объект',\n",
       "  'культурного',\n",
       "  'наследия',\n",
       "  'народов',\n",
       "  'России']]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_chunk = n_sentenize(text1)\n",
    "n_sen_chunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "53863dae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['杭州因风景秀丽，素有',\n",
       "  '“',\n",
       "  '人间天堂',\n",
       "  '”',\n",
       "  '的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。']]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_sen_chunk_2 = n_sentenize(text2)\n",
    "n_sen_chunk_2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bed3aa8b",
   "metadata": {},
   "source": [
    "# Частеречная разметка"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e1be4d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "from navec import Navec\n",
    "from slovnet import Morph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e67efcf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "2d596319",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл необходимо скачать по ссылке https://github.com/natasha/navec#downloads\n",
    "navec = Navec.load(r'C:\\Users\\asus\\Desktop\\iu5\\MMO\\lab5\\navec_news_v1_1B_250K_300d_100q.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "175f21b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Файл необходимо скачать по ссылке https://github.com/natasha/slovnet#downloads\n",
    "n_morph = Morph.load(r'C:\\Users\\asus\\Desktop\\iu5\\MMO\\lab5\\slovnet_morph_news_v1.tar', batch_size=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "917e0521",
   "metadata": {},
   "outputs": [],
   "source": [
    "morph_res = n_morph.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "d274e793",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_pos(markup):\n",
    "    for token in markup.tokens:\n",
    "        print('{} - {}'.format(token.text, token.tag))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "757e5d64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Моско́вский - PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
      "госуда́рственный - X|Foreign=Yes\n",
      "техни́ческий - ADJ|Case=Gen|Degree=Pos|Gender=Masc|Number=Sing\n",
      "университе́т - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
      "им - PRON|Case=Ins|Gender=Masc|Number=Sing|Person=3\n",
      ". - PUNCT\n",
      "Н - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      ". - PUNCT\n",
      "Э - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      ". - PUNCT\n",
      "Ба́умана - PROPN|Animacy=Anim|Case=Gen|Gender=Masc|Number=Sing\n",
      "[ - PUNCT\n",
      "a - X|Foreign=Yes\n",
      "] - PUNCT\n",
      "( - PUNCT\n",
      "также - ADV|Degree=Pos\n",
      "известен - ADJ|Degree=Pos|Gender=Masc|Number=Sing|Variant=Short\n",
      "как - SCONJ\n",
      "Ба́уманка - PROPN|Animacy=Anim|Case=Nom|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "Ба́уманский - PROPN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "МГТУ - PROPN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
      ", - PUNCT\n",
      "МВТУ - PROPN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Sing\n",
      ") - PUNCT\n",
      "— - PUNCT\n",
      "российский - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
      "национальный - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
      "исследовательский - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
      "университет - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "научный - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
      "центр - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      ", - PUNCT\n",
      "особо - ADV|Degree=Pos\n",
      "ценный - ADJ|Case=Nom|Degree=Pos|Gender=Masc|Number=Sing\n",
      "объект - NOUN|Animacy=Inan|Case=Nom|Gender=Masc|Number=Sing\n",
      "культурного - ADJ|Case=Gen|Degree=Pos|Gender=Neut|Number=Sing\n",
      "наследия - NOUN|Animacy=Inan|Case=Gen|Gender=Neut|Number=Sing\n",
      "народов - NOUN|Animacy=Inan|Case=Gen|Gender=Masc|Number=Plur\n",
      "России - PROPN|Animacy=Inan|Case=Gen|Gender=Fem|Number=Sing\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_text_markup = list(_ for _ in n_morph.map(n_sen_chunk))\n",
    "[print_pos(x) for x in n_text_markup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "5b39f24e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭州因风景秀丽，素有 - PUNCT\n",
      "“ - PUNCT\n",
      "人间天堂 - PUNCT\n",
      "” - PUNCT\n",
      "的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。 - PUNCT\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[None]"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_text2_markup = list(n_morph.map(n_sen_chunk_2))\n",
    "[print_pos(x) for x in n_text2_markup]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "97e19079",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import Doc, Segmenter, NewsEmbedding, NewsMorphTagger, MorphVocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "31beea5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#词性还原\n",
    "def n_lemmatize(text):\n",
    "    emb = NewsEmbedding()\n",
    "    morph_tagger = NewsMorphTagger(emb)\n",
    "    segmenter = Segmenter()\n",
    "    morph_vocab = MorphVocab()\n",
    "    doc = Doc(text)\n",
    "    doc.segment(segmenter)\n",
    "    doc.tag_morph(morph_tagger)\n",
    "    for token in doc.tokens:\n",
    "        token.lemmatize(morph_vocab)\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "0e3befee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Моско́вский': 'моско́вский',\n",
       " 'госуда́рственный': 'госуда́рственный',\n",
       " 'техни́ческий': 'техни́ческий',\n",
       " 'университе́т': 'университе́т',\n",
       " 'им': 'он',\n",
       " '.': '.',\n",
       " 'Н': 'н',\n",
       " 'Э': 'э',\n",
       " 'Ба́умана': 'ба́уман',\n",
       " '[': '[',\n",
       " 'a': 'a',\n",
       " ']': ']',\n",
       " '(': '(',\n",
       " 'также': 'также',\n",
       " 'известен': 'известный',\n",
       " 'как': 'как',\n",
       " 'Ба́уманка': 'ба́уманка',\n",
       " ',': ',',\n",
       " 'Ба́уманский': 'ба́уманский',\n",
       " 'МГТУ': 'мгту',\n",
       " 'МВТУ': 'мвт',\n",
       " ')': ')',\n",
       " '—': '—',\n",
       " 'российский': 'российский',\n",
       " 'национальный': 'национальный',\n",
       " 'исследовательский': 'исследовательский',\n",
       " 'университет': 'университет',\n",
       " 'научный': 'научный',\n",
       " 'центр': 'центр',\n",
       " 'особо': 'особо',\n",
       " 'ценный': 'ценный',\n",
       " 'объект': 'объект',\n",
       " 'культурного': 'культурный',\n",
       " 'наследия': 'наследие',\n",
       " 'народов': 'народ',\n",
       " 'России': 'россия'}"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc = n_lemmatize(text1)\n",
    "{_.text: _.lemma for _ in n_doc.tokens}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "edb6b1d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'杭州因风景秀丽，素有': '杭州因风景秀丽，素有',\n",
       " '“': '“',\n",
       " '人间天堂': '人间天堂',\n",
       " '”': '”',\n",
       " '的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。': '的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。'}"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_doc2 = n_lemmatize(text2)\n",
    "{_.text: _.lemma for _ in n_doc2.tokens}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eee281d",
   "metadata": {},
   "source": [
    "# Выделение (распознавание) именованных сущностей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "efb0be13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#name entity tagging (命名实体标记)\n",
    "from slovnet import NER\n",
    "from ipymarkup import show_span_ascii_markup as show_markup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "bc01aef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner = NER.load(r'C:\\Users\\asus\\Desktop\\iu5\\MMO\\lab5\\slovnet_ner_news_v1.tar')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "fc7dda09",
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_res = ner.navec(navec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "e4e10020",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SpanMarkup(\n",
       "    text='杭州因风景秀丽，素有“人间天堂”的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。',\n",
       "    spans=[]\n",
       ")"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "markup_ner = ner(text2)\n",
    "markup_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "3b425df5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "杭州因风景秀丽，素有“人间天堂”的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新\n",
      "世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。\n"
     ]
    }
   ],
   "source": [
    "show_markup(markup_ner.text, markup_ner.spans)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ba6998",
   "metadata": {},
   "source": [
    "# Разбор предложения（语法解析）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "220800bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from natasha import NewsSyntaxParser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "12dc1a47",
   "metadata": {},
   "outputs": [],
   "source": [
    "emb = NewsEmbedding()\n",
    "syntax_parser = NewsSyntaxParser(emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b0727390",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ┌────────────────────────► Моско́вский       amod\n",
      "  │ ┌─┌────────────────────► госуда́рственный  amod\n",
      "  │ │ │ ┌──────────────────► техни́ческий      amod\n",
      "┌─│ │ │ │                 ┌─ университе́т      \n",
      "│ │ │ │ │               ┌─└► им                nmod\n",
      "│ │ │ │ │               │ ┌► .                 punct\n",
      "│ │ │ │ │           ┌─┌─└►└─ Н                 nmod\n",
      "│ │ │ │ │           │ │ └──► .                 punct\n",
      "│ │ │ │ │           │ └────► Э                 flat:name\n",
      "│ │ │ │ │           │   ┌──► .                 punct\n",
      "│ │ │ │ │           └──►│    Ба́умана          flat:name\n",
      "│ │ │ │ │               │ ┌► [                 punct\n",
      "│ │ │ │ │               └─└─ a                 \n",
      "│ │ │ │ │               └──► ]                 punct\n",
      "│ │ │ │ │               ┌──► (                 punct\n",
      "│ │ │ │ │               │ ┌► также             advmod\n",
      "│ │ │ │ │   ┌►┌───────┌─└─└─ известен          parataxis\n",
      "│ │ │ │ │   │ │       │   ┌► как               case\n",
      "│ │ │ │ │ ┌─│ │ ┌─┌─┌─└──►└─ Ба́уманка         obl\n",
      "│ │ │ │ │ │ │ │ │ │ │     ┌► ,                 punct\n",
      "│ │ │ │ │ │ │ │ │ │ └────►└─ Ба́уманский       conj\n",
      "│ │ │ │ │ │ │ │ │ │       ┌► ,                 punct\n",
      "│ │ │ │ │ │ │ │ │ └──────►└─ МГТУ              conj\n",
      "│ │ │ │ │ │ │ │ │         ┌► ,                 punct\n",
      "│ │ │ │ │ │ │ │ └────────►└─ МВТУ              conj\n",
      "│ │ │ │ │ │ │ └────────────► )                 punct\n",
      "│ │ │ │ │ │ │     ┌────────► —                 punct\n",
      "│ │ │ │ │ │ │     │   ┌────► российский        amod\n",
      "│ │ │ │ │ │ │     │   │ ┌──► национальный      amod\n",
      "│ │ │ │ │ │ │     │   │ │ ┌► исследовательский amod\n",
      "│ └─└►└─└─│ └─────│ ┌─└─└─└─ университет       obj\n",
      "│         │       │ │   ┌──► ,                 punct\n",
      "│         │       │ │   │ ┌► научный           amod\n",
      "│         │       │ └──►└─└─ центр             conj\n",
      "│         │       │     ┌──► ,                 punct\n",
      "│         │       │     │ ┌► особо             advmod\n",
      "│         └──────►│     │ └─ ценный            conj\n",
      "└────────────────►└─────└─── объект            obj\n",
      "                        │ ┌► культурного       amod\n",
      "                        └►└─ наследия          nmod\n",
      "                        └►┌─ народов           nmod\n",
      "                          └► России            nmod\n"
     ]
    }
   ],
   "source": [
    "n_doc.parse_syntax(syntax_parser)\n",
    "n_doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "96bdf507",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ┌────────────────────────► Моско́вский       amod\n",
      "  │ ┌─┌────────────────────► госуда́рственный  amod\n",
      "  │ │ │ ┌──────────────────► техни́ческий      amod\n",
      "┌─│ │ │ │                 ┌─ университе́т      \n",
      "│ │ │ │ │               ┌─└► им                nmod\n",
      "│ │ │ │ │               │ ┌► .                 punct\n",
      "│ │ │ │ │           ┌─┌─└►└─ Н                 nmod\n",
      "│ │ │ │ │           │ │ └──► .                 punct\n",
      "│ │ │ │ │           │ └────► Э                 flat:name\n",
      "│ │ │ │ │           │   ┌──► .                 punct\n",
      "│ │ │ │ │           └──►│    Ба́умана          flat:name\n",
      "│ │ │ │ │               │ ┌► [                 punct\n",
      "│ │ │ │ │               └─└─ a                 \n",
      "│ │ │ │ │               └──► ]                 punct\n",
      "│ │ │ │ │               ┌──► (                 punct\n",
      "│ │ │ │ │               │ ┌► также             advmod\n",
      "│ │ │ │ │   ┌►┌───────┌─└─└─ известен          parataxis\n",
      "│ │ │ │ │   │ │       │   ┌► как               case\n",
      "│ │ │ │ │ ┌─│ │ ┌─┌─┌─└──►└─ Ба́уманка         obl\n",
      "│ │ │ │ │ │ │ │ │ │ │     ┌► ,                 punct\n",
      "│ │ │ │ │ │ │ │ │ │ └────►└─ Ба́уманский       conj\n",
      "│ │ │ │ │ │ │ │ │ │       ┌► ,                 punct\n",
      "│ │ │ │ │ │ │ │ │ └──────►└─ МГТУ              conj\n",
      "│ │ │ │ │ │ │ │ │         ┌► ,                 punct\n",
      "│ │ │ │ │ │ │ │ └────────►└─ МВТУ              conj\n",
      "│ │ │ │ │ │ │ └────────────► )                 punct\n",
      "│ │ │ │ │ │ │     ┌────────► —                 punct\n",
      "│ │ │ │ │ │ │     │   ┌────► российский        amod\n",
      "│ │ │ │ │ │ │     │   │ ┌──► национальный      amod\n",
      "│ │ │ │ │ │ │     │   │ │ ┌► исследовательский amod\n",
      "│ └─└►└─└─│ └─────│ ┌─└─└─└─ университет       obj\n",
      "│         │       │ │   ┌──► ,                 punct\n",
      "│         │       │ │   │ ┌► научный           amod\n",
      "│         │       │ └──►└─└─ центр             conj\n",
      "│         │       │     ┌──► ,                 punct\n",
      "│         │       │     │ ┌► особо             advmod\n",
      "│         └──────►│     │ └─ ценный            conj\n",
      "└────────────────►└─────└─── объект            obj\n",
      "                        │ ┌► культурного       amod\n",
      "                        └►└─ наследия          nmod\n",
      "                        └►┌─ народов           nmod\n",
      "                          └► России            nmod\n"
     ]
    }
   ],
   "source": [
    "n_doc.parse_syntax(syntax_parser)\n",
    "n_doc.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "ea68b609",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       杭州因风景秀丽，素有                                                                                   \n",
      "    ┌► “                                                                                            punct\n",
      "┌─┌─└─ 人间天堂                                                                                         \n",
      "│ └──► ”                                                                                            punct\n",
      "└────► 的美誉。杭州得益于京杭大运河和通商口岸的便利，以及自身发达的丝绸和粮食产业，历史上曾是重要的商业集散中心。新世纪以来，随着阿里巴巴等高科技企业的带动，互联网经济成为杭州新的经济增长点。 punct\n"
     ]
    }
   ],
   "source": [
    "n_doc2.parse_syntax(syntax_parser)\n",
    "n_doc2.sents[0].syntax.print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e7d2fd9",
   "metadata": {},
   "source": [
    "# Векторизация текста на основе модели \"мешка слов\"（bag of words）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "af2bfa17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, mean_squared_log_error, median_absolute_error, r2_score \n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from collections import Counter\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "\n",
    "%matplotlib inline \n",
    "sns.set(style=\"ticks\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "87d6bf56",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"rec.motorcycles\", \"rec.sport.baseball\", \"sci.electronics\",\"sci.med\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "0d16c6f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "e86af72b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество сформированных признаков - 33448\n"
     ]
    }
   ],
   "source": [
    "vocabVect = CountVectorizer()\n",
    "vocabVect.fit(data)\n",
    "corpusVocab = vocabVect.vocabulary_\n",
    "print('Количество сформированных признаков - {}'.format(len(corpusVocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "bef87c84",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrmendel=22213\n",
      "unix=31462\n",
      "amherst=5287\n",
      "edu=12444\n",
      "nathaniel=21624\n",
      "mendell=20477\n",
      "subject=29220\n",
      "re=25369\n",
      "bike=6898\n"
     ]
    }
   ],
   "source": [
    "for i in list(corpusVocab)[1:10]:\n",
    "    print('{}={}'.format(i, corpusVocab[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e797a6",
   "metadata": {},
   "source": [
    "# Использование класса CountVectorizer(使用 CountVectorizer 类)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "20e16009",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<2380x33448 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 335176 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features = vocabVect.transform(data)\n",
    "test_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "62d10431",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "matrix([[0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [2, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0],\n",
       "        [0, 0, 0, ..., 0, 0, 0]], dtype=int64)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_features.todense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "70c5e0d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33448"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Размер нулевой строки\n",
    "len(test_features.todense()[0].getA1())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "1d428148",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 1, 1, 1, 1, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 3, 2]\n"
     ]
    }
   ],
   "source": [
    "print([i for i in test_features.todense()[0].getA1() if i>0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "81f5dbe7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['00',\n",
       " '000',\n",
       " '0000',\n",
       " '0000000004',\n",
       " '0000000005',\n",
       " '0000000667',\n",
       " '0000001200',\n",
       " '0001',\n",
       " '00014',\n",
       " '0002']"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocabVect.get_feature_names()[0:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fcb594a",
   "metadata": {},
   "source": [
    "# Решение задачи анализа тональности текста на основе модели \"мешка слов\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "77d9de92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def VectorizeAndClassify(vectorizers_list, classifiers_list):\n",
    "    for v in vectorizers_list:\n",
    "        for c in classifiers_list:\n",
    "            pipeline1 = Pipeline([(\"vectorizer\", v), (\"classifier\", c)])\n",
    "            score = cross_val_score(pipeline1, newsgroups['data'], newsgroups['target'], scoring='accuracy', cv=3).mean()\n",
    "            print('Векторизация - {}'.format(v))\n",
    "            print('Модель для классификации - {}'.format(c))\n",
    "            print('Accuracy = {}'.format(score))\n",
    "            print('===========================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "e94ab10e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
      "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
      "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
      "                            '0005111312': 11, '0005111312na1em': 12,\n",
      "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
      "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
      "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
      "                            '001813': 24, '002': 25, '002222': 26,\n",
      "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
      "Модель для классификации - LogisticRegression(C=3.0)\n",
      "Accuracy = 0.9382336841146768\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
      "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
      "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
      "                            '0005111312': 11, '0005111312na1em': 12,\n",
      "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
      "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
      "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
      "                            '001813': 24, '002': 25, '002222': 26,\n",
      "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
      "Модель для классификации - LinearSVC()\n",
      "Accuracy = 0.9453742497059174\n",
      "===========================\n",
      "Векторизация - CountVectorizer(vocabulary={'00': 0, '000': 1, '0000': 2, '0000000004': 3,\n",
      "                            '0000000005': 4, '0000000667': 5, '0000001200': 6,\n",
      "                            '0001': 7, '00014': 8, '0002': 9, '0003': 10,\n",
      "                            '0005111312': 11, '0005111312na1em': 12,\n",
      "                            '00072': 13, '000851': 14, '000rpm': 15,\n",
      "                            '000th': 16, '001': 17, '0010': 18, '001004': 19,\n",
      "                            '0011': 20, '001211': 21, '0013': 22, '001642': 23,\n",
      "                            '001813': 24, '002': 25, '002222': 26,\n",
      "                            '002251w': 27, '0023': 28, '002937': 29, ...})\n",
      "Модель для классификации - KNeighborsClassifier()\n",
      "Accuracy = 0.6655358653541747\n",
      "===========================\n"
     ]
    }
   ],
   "source": [
    "vectorizers_list = [CountVectorizer(vocabulary = corpusVocab)]\n",
    "classifiers_list = [LogisticRegression(C=3.0), LinearSVC(), KNeighborsClassifier()]\n",
    "VectorizeAndClassify(vectorizers_list, classifiers_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b2be42",
   "metadata": {},
   "source": [
    "# Разделим выборку на обучающую и тестовую и проверим решение для лучшей модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "41293c55",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(newsgroups['data'], newsgroups['target'], test_size=0.5, random_state=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "759cc58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "8324a6a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.9290322580645162\n",
      "1 \t 0.9675090252707581\n",
      "2 \t 0.9026845637583892\n",
      "3 \t 0.9245901639344263\n"
     ]
    }
   ],
   "source": [
    "sentiment(CountVectorizer(), LinearSVC())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7015e19",
   "metadata": {},
   "source": [
    "# Работа с векторными представлениями слов с использованием word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "5a54557c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting gensim\n",
      "  Using cached gensim-4.2.0-cp39-cp39-win_amd64.whl (23.9 MB)\n",
      "Requirement already satisfied: smart-open>=1.8.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (5.2.1)\n",
      "Requirement already satisfied: numpy>=1.17.0 in c:\\users\\asus\\appdata\\roaming\\python\\python39\\site-packages (from gensim) (1.22.3)\n",
      "Collecting Cython==0.29.28\n",
      "  Using cached Cython-0.29.28-py2.py3-none-any.whl (983 kB)\n",
      "Requirement already satisfied: scipy>=0.18.1 in c:\\programdata\\anaconda3\\lib\\site-packages (from gensim) (1.7.1)\n",
      "Installing collected packages: Cython, gensim\n",
      "Successfully installed Cython-0.29.28 gensim-4.2.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "  WARNING: The scripts cygdb.exe, cython.exe and cythonize.exe are installed in 'C:\\Users\\asus\\AppData\\Roaming\\Python\\Python39\\Scripts' which is not on PATH.\n",
      "  Consider adding this directory to PATH or, if you prefer to suppress this warning, use --no-warn-script-location.\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -umpy (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution -raitlets (c:\\programdata\\anaconda3\\lib\\site-packages)\n",
      "WARNING: Ignoring invalid distribution - (c:\\programdata\\anaconda3\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip install --user gensim\n",
    "import gensim\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "a066f573",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('ruscorpora_mystem_cbow_300_2_2015.bin.gz',\n",
       " <http.client.HTTPMessage at 0x254e2cceaf0>)"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "urllib.request.urlretrieve(\"http://rusvectores.org/static/models/rusvectores2/ruscorpora_mystem_cbow_300_2_2015.bin.gz\", \"ruscorpora_mystem_cbow_300_2_2015.bin.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "6144ab9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = 'ruscorpora_mystem_cbow_300_2_2015.bin.gz'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "5aa69a42",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format(model_path, binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "8865dc3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "words = ['холод_S', 'мороз_S', 'береза_S', 'сосна_S']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "0bdc6909",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "СЛОВО - холод_S\n",
      "5 ближайших соседей слова:\n",
      "стужа_S => 0.7676383852958679\n",
      "сырость_S => 0.6338975429534912\n",
      "жара_S => 0.6089427471160889\n",
      "мороз_S => 0.5890367031097412\n",
      "озноб_S => 0.5776054859161377\n",
      "\n",
      "СЛОВО - мороз_S\n",
      "5 ближайших соседей слова:\n",
      "стужа_S => 0.6425479650497437\n",
      "морозец_S => 0.5947279930114746\n",
      "холод_S => 0.5890367031097412\n",
      "жара_S => 0.5522176623344421\n",
      "снегопад_S => 0.5083199143409729\n",
      "\n",
      "СЛОВО - береза_S\n",
      "5 ближайших соседей слова:\n",
      "сосна_S => 0.7943247556686401\n",
      "тополь_S => 0.7562226057052612\n",
      "дуб_S => 0.7440178394317627\n",
      "дерево_S => 0.7373415231704712\n",
      "клен_S => 0.7105200290679932\n",
      "\n",
      "СЛОВО - сосна_S\n",
      "5 ближайших соседей слова:\n",
      "береза_S => 0.7943247556686401\n",
      "дерево_S => 0.7581434845924377\n",
      "лиственница_S => 0.747814953327179\n",
      "дуб_S => 0.7412480711936951\n",
      "ель_S => 0.7363824248313904\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    if word in model:\n",
    "        print('\\nСЛОВО - {}'.format(word))\n",
    "        print('5 ближайших соседей слова:')\n",
    "        for word, sim in model.most_similar(positive=[word], topn=5):\n",
    "            print('{} => {}'.format(word, sim))\n",
    "    else:\n",
    "        print('Слово \"{}\" не найдено в модели'.format(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e23be841",
   "metadata": {},
   "source": [
    "# Находим близость между словами и строим аналогии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "eb8eda3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('сырость_S', 0.5040211081504822), ('стылость_S', 0.46336129307746887), ('голод_S', 0.4604816436767578), ('зной_S', 0.45904627442359924), ('скука_S', 0.4489358067512512), ('жара_S', 0.44645121693611145), ('усталость_S', 0.4218570291996002), ('озноб_S', 0.41469818353652954), ('духота_S', 0.4099087715148926), ('неуют_S', 0.40298789739608765)]\n"
     ]
    }
   ],
   "source": [
    "print(model.most_similar(positive=['холод_S', 'стужа_S'], negative=['мороз_S']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97aadb66",
   "metadata": {},
   "source": [
    "# Обучим word2vec на наборе данных \"fetch_20newsgroups\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "1dd818c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\asus\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from nltk import WordPunctTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "2dd72f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = [\"rec.motorcycles\", \"rec.sport.baseball\", \"sci.electronics\",\"sci.med\"]\n",
    "newsgroups = fetch_20newsgroups(subset='train', categories=categories)\n",
    "data = newsgroups['data']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "86017119",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Подготовим корпус\n",
    "corpus = []\n",
    "stop_words = stopwords.words('english')\n",
    "tok = WordPunctTokenizer()\n",
    "for line in newsgroups['data']:\n",
    "    line1 = line.strip().lower()\n",
    "    line1 = re.sub(\"[^a-zA-Z]\",\" \", line1)\n",
    "    text_tok = tok.tokenize(line1)\n",
    "    text_tok1 = [w for w in text_tok if not w in stop_words]\n",
    "    corpus.append(text_tok1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "8db26353",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['nrmendel',\n",
       "  'unix',\n",
       "  'amherst',\n",
       "  'edu',\n",
       "  'nathaniel',\n",
       "  'mendell',\n",
       "  'subject',\n",
       "  'bike',\n",
       "  'advice',\n",
       "  'organization',\n",
       "  'amherst',\n",
       "  'college',\n",
       "  'x',\n",
       "  'newsreader',\n",
       "  'tin',\n",
       "  'version',\n",
       "  'pl',\n",
       "  'lines',\n",
       "  'ummm',\n",
       "  'bikes',\n",
       "  'kx',\n",
       "  'suggest',\n",
       "  'look',\n",
       "  'zx',\n",
       "  'since',\n",
       "  'horsepower',\n",
       "  'whereas',\n",
       "  'might',\n",
       "  'bit',\n",
       "  'much',\n",
       "  'sincerely',\n",
       "  'nathaniel',\n",
       "  'zx',\n",
       "  'dod',\n",
       "  'ama'],\n",
       " ['grante',\n",
       "  'aquarius',\n",
       "  'rosemount',\n",
       "  'com',\n",
       "  'grant',\n",
       "  'edwards',\n",
       "  'subject',\n",
       "  'krillean',\n",
       "  'photography',\n",
       "  'reply',\n",
       "  'grante',\n",
       "  'aquarius',\n",
       "  'rosemount',\n",
       "  'com',\n",
       "  'grant',\n",
       "  'edwards',\n",
       "  'organization',\n",
       "  'rosemount',\n",
       "  'inc',\n",
       "  'lines',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'aquarius',\n",
       "  'stgprao',\n",
       "  'st',\n",
       "  'unocal',\n",
       "  'com',\n",
       "  'richard',\n",
       "  'ottolini',\n",
       "  'writes',\n",
       "  'living',\n",
       "  'things',\n",
       "  'maintain',\n",
       "  'small',\n",
       "  'electric',\n",
       "  'fields',\n",
       "  'enhance',\n",
       "  'certain',\n",
       "  'chemical',\n",
       "  'reactions',\n",
       "  'promote',\n",
       "  'communication',\n",
       "  'states',\n",
       "  'cell',\n",
       "  'communicate',\n",
       "  'cells',\n",
       "  'nervous',\n",
       "  'system',\n",
       "  'specialized',\n",
       "  'example',\n",
       "  'perhaps',\n",
       "  'uses',\n",
       "  'true',\n",
       "  'electric',\n",
       "  'fields',\n",
       "  'change',\n",
       "  'location',\n",
       "  'time',\n",
       "  'large',\n",
       "  'organism',\n",
       "  'also',\n",
       "  'true',\n",
       "  'special',\n",
       "  'photographic',\n",
       "  'techniques',\n",
       "  'applying',\n",
       "  'external',\n",
       "  'fields',\n",
       "  'kirillian',\n",
       "  'photography',\n",
       "  'interact',\n",
       "  'fields',\n",
       "  'resistances',\n",
       "  'caused',\n",
       "  'fields',\n",
       "  'make',\n",
       "  'interesting',\n",
       "  'pictures',\n",
       "  'really',\n",
       "  'kirlian',\n",
       "  'photography',\n",
       "  'taking',\n",
       "  'pictures',\n",
       "  'corona',\n",
       "  'discharge',\n",
       "  'objects',\n",
       "  'animate',\n",
       "  'inanimate',\n",
       "  'fields',\n",
       "  'applied',\n",
       "  'objects',\n",
       "  'millions',\n",
       "  'times',\n",
       "  'larger',\n",
       "  'biologically',\n",
       "  'created',\n",
       "  'fields',\n",
       "  'want',\n",
       "  'record',\n",
       "  'biologically',\n",
       "  'created',\n",
       "  'electric',\n",
       "  'fields',\n",
       "  'got',\n",
       "  'use',\n",
       "  'low',\n",
       "  'noise',\n",
       "  'high',\n",
       "  'gain',\n",
       "  'sensors',\n",
       "  'typical',\n",
       "  'eegs',\n",
       "  'ekgs',\n",
       "  'kirlian',\n",
       "  'photography',\n",
       "  'phun',\n",
       "  'physics',\n",
       "  'type',\n",
       "  'stuff',\n",
       "  'right',\n",
       "  'soaking',\n",
       "  'chunks',\n",
       "  'extra',\n",
       "  'fine',\n",
       "  'steel',\n",
       "  'wool',\n",
       "  'liquid',\n",
       "  'oxygen',\n",
       "  'hitting',\n",
       "  'hammer',\n",
       "  'like',\n",
       "  'kirlean',\n",
       "  'setup',\n",
       "  'fun',\n",
       "  'possibly',\n",
       "  'dangerous',\n",
       "  'perhaps',\n",
       "  'pictures',\n",
       "  'diagonistic',\n",
       "  'disease',\n",
       "  'problems',\n",
       "  'organisms',\n",
       "  'better',\n",
       "  'understood',\n",
       "  'perhaps',\n",
       "  'probably',\n",
       "  'grant',\n",
       "  'edwards',\n",
       "  'yow',\n",
       "  'vote',\n",
       "  'rosemount',\n",
       "  'inc',\n",
       "  'well',\n",
       "  'tapered',\n",
       "  'half',\n",
       "  'cocked',\n",
       "  'ill',\n",
       "  'conceived',\n",
       "  'grante',\n",
       "  'aquarius',\n",
       "  'rosemount',\n",
       "  'com',\n",
       "  'tax',\n",
       "  'deferred'],\n",
       " ['liny',\n",
       "  'sun',\n",
       "  'scri',\n",
       "  'fsu',\n",
       "  'edu',\n",
       "  'nemo',\n",
       "  'subject',\n",
       "  'bates',\n",
       "  'method',\n",
       "  'myopia',\n",
       "  'reply',\n",
       "  'lin',\n",
       "  'ray',\n",
       "  'met',\n",
       "  'fsu',\n",
       "  'edu',\n",
       "  'distribution',\n",
       "  'na',\n",
       "  'organization',\n",
       "  'scri',\n",
       "  'florida',\n",
       "  'state',\n",
       "  'university',\n",
       "  'lines',\n",
       "  'bates',\n",
       "  'method',\n",
       "  'work',\n",
       "  'first',\n",
       "  'heard',\n",
       "  'newsgroup',\n",
       "  'several',\n",
       "  'years',\n",
       "  'ago',\n",
       "  'got',\n",
       "  'hold',\n",
       "  'book',\n",
       "  'improve',\n",
       "  'sight',\n",
       "  'simple',\n",
       "  'daily',\n",
       "  'drills',\n",
       "  'relaxation',\n",
       "  'margaret',\n",
       "  'corbett',\n",
       "  'authorized',\n",
       "  'instructor',\n",
       "  'bates',\n",
       "  'method',\n",
       "  'published',\n",
       "  'talks',\n",
       "  'vision',\n",
       "  'improvement',\n",
       "  'relaxation',\n",
       "  'exercise',\n",
       "  'study',\n",
       "  'whether',\n",
       "  'method',\n",
       "  'actually',\n",
       "  'works',\n",
       "  'works',\n",
       "  'actually',\n",
       "  'shortening',\n",
       "  'previously',\n",
       "  'elongated',\n",
       "  'eyeball',\n",
       "  'increasing',\n",
       "  'lens',\n",
       "  'ability',\n",
       "  'flatten',\n",
       "  'order',\n",
       "  'compensate',\n",
       "  'long',\n",
       "  'eyeball',\n",
       "  'since',\n",
       "  'myopia',\n",
       "  'result',\n",
       "  'eyeball',\n",
       "  'elongation',\n",
       "  'seems',\n",
       "  'logical',\n",
       "  'approach',\n",
       "  'correction',\n",
       "  'find',\n",
       "  'way',\n",
       "  'reverse',\n",
       "  'process',\n",
       "  'e',\n",
       "  'shorten',\n",
       "  'somehow',\n",
       "  'preferably',\n",
       "  'non',\n",
       "  'surgically',\n",
       "  'recent',\n",
       "  'studies',\n",
       "  'find',\n",
       "  'know',\n",
       "  'rk',\n",
       "  'works',\n",
       "  'changing',\n",
       "  'curvature',\n",
       "  'cornea',\n",
       "  'compensate',\n",
       "  'shape',\n",
       "  'eyeball',\n",
       "  'way',\n",
       "  'train',\n",
       "  'muscles',\n",
       "  'shorten',\n",
       "  'eyeball',\n",
       "  'back',\n",
       "  'correct',\n",
       "  'length',\n",
       "  'would',\n",
       "  'even',\n",
       "  'better',\n",
       "  'bates',\n",
       "  'idea',\n",
       "  'right',\n",
       "  'thanks',\n",
       "  'information'],\n",
       " ['mcovingt',\n",
       "  'aisun',\n",
       "  'ai',\n",
       "  'uga',\n",
       "  'edu',\n",
       "  'michael',\n",
       "  'covington',\n",
       "  'subject',\n",
       "  'buy',\n",
       "  'parts',\n",
       "  'time',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'aisun',\n",
       "  'ai',\n",
       "  'uga',\n",
       "  'edu',\n",
       "  'organization',\n",
       "  'ai',\n",
       "  'programs',\n",
       "  'university',\n",
       "  'georgia',\n",
       "  'athens',\n",
       "  'lines',\n",
       "  'pricing',\n",
       "  'parts',\n",
       "  'reminds',\n",
       "  'something',\n",
       "  'chemist',\n",
       "  'said',\n",
       "  'gram',\n",
       "  'dye',\n",
       "  'costs',\n",
       "  'dollar',\n",
       "  'comes',\n",
       "  'liter',\n",
       "  'jar',\n",
       "  'also',\n",
       "  'costs',\n",
       "  'dollar',\n",
       "  'want',\n",
       "  'whole',\n",
       "  'barrel',\n",
       "  'also',\n",
       "  'costs',\n",
       "  'dollar',\n",
       "  'e',\n",
       "  'charge',\n",
       "  'almost',\n",
       "  'exclusively',\n",
       "  'packaging',\n",
       "  'delivering',\n",
       "  'chemical',\n",
       "  'particular',\n",
       "  'case',\n",
       "  'byproduct',\n",
       "  'cost',\n",
       "  'almost',\n",
       "  'nothing',\n",
       "  'intrinsically',\n",
       "  'michael',\n",
       "  'covington',\n",
       "  'associate',\n",
       "  'research',\n",
       "  'scientist',\n",
       "  'artificial',\n",
       "  'intelligence',\n",
       "  'programs',\n",
       "  'mcovingt',\n",
       "  'ai',\n",
       "  'uga',\n",
       "  'edu',\n",
       "  'university',\n",
       "  'georgia',\n",
       "  'phone',\n",
       "  'athens',\n",
       "  'georgia',\n",
       "  'u',\n",
       "  'amateur',\n",
       "  'radio',\n",
       "  'n',\n",
       "  'tmi'],\n",
       " ['tammy',\n",
       "  'vandenboom',\n",
       "  'launchpad',\n",
       "  'unc',\n",
       "  'edu',\n",
       "  'tammy',\n",
       "  'vandenboom',\n",
       "  'subject',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'testicles',\n",
       "  'nntp',\n",
       "  'posting',\n",
       "  'host',\n",
       "  'lambada',\n",
       "  'oit',\n",
       "  'unc',\n",
       "  'edu',\n",
       "  'organization',\n",
       "  'university',\n",
       "  'north',\n",
       "  'carolina',\n",
       "  'extended',\n",
       "  'bulletin',\n",
       "  'board',\n",
       "  'service',\n",
       "  'distribution',\n",
       "  'na',\n",
       "  'lines',\n",
       "  'husband',\n",
       "  'woke',\n",
       "  'three',\n",
       "  'days',\n",
       "  'ago',\n",
       "  'small',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'spot',\n",
       "  'size',\n",
       "  'nickel',\n",
       "  'one',\n",
       "  'testicles',\n",
       "  'bottom',\n",
       "  'side',\n",
       "  'knots',\n",
       "  'lumps',\n",
       "  'little',\n",
       "  'sore',\n",
       "  'spot',\n",
       "  'says',\n",
       "  'reminds',\n",
       "  'bruise',\n",
       "  'feels',\n",
       "  'recollection',\n",
       "  'hitting',\n",
       "  'anything',\n",
       "  'like',\n",
       "  'would',\n",
       "  'cause',\n",
       "  'bruise',\n",
       "  'asssures',\n",
       "  'remember',\n",
       "  'something',\n",
       "  'like',\n",
       "  'clues',\n",
       "  'might',\n",
       "  'somewhat',\n",
       "  'hypochondriac',\n",
       "  'sp',\n",
       "  'sure',\n",
       "  'gonna',\n",
       "  'die',\n",
       "  'thanks',\n",
       "  'opinions',\n",
       "  'expressed',\n",
       "  'necessarily',\n",
       "  'university',\n",
       "  'north',\n",
       "  'carolina',\n",
       "  'chapel',\n",
       "  'hill',\n",
       "  'campus',\n",
       "  'office',\n",
       "  'information',\n",
       "  'technology',\n",
       "  'experimental',\n",
       "  'bulletin',\n",
       "  'board',\n",
       "  'service',\n",
       "  'internet',\n",
       "  'launchpad',\n",
       "  'unc',\n",
       "  'edu']]"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "72bb9e03",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 1.99 s\n"
     ]
    }
   ],
   "source": [
    "%time model_imdb = word2vec.Word2Vec(corpus, workers=4, min_count=10, window=10, sample=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "e32faafe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('want', 0.98796147108078), ('work', 0.9793075919151306), ('etc', 0.9789608716964722), ('used', 0.9754022359848022), ('using', 0.9715811014175415)]\n"
     ]
    }
   ],
   "source": [
    "# Проверим, что модель обучилась\n",
    "print(model_imdb.wv.most_similar(positive=['find'], topn=5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "4e251284",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_2(v, c):\n",
    "    model = Pipeline(\n",
    "        [(\"vectorizer\", v), \n",
    "         (\"classifier\", c)])\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print_accuracy_score_for_classes(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26e3d095",
   "metadata": {},
   "source": [
    "# Проверка качества работы модели word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "16bfea34",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmbeddingVectorizer(object):\n",
    "    '''\n",
    "    Для текста усредним вектора входящих в него слов\n",
    "    '''\n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "        self.size = model.vector_size\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        return self\n",
    "\n",
    "    def transform(self, X):\n",
    "        return np.array([np.mean(\n",
    "            [self.model[w] for w in words if w in self.model] \n",
    "            or [np.zeros(self.size)], axis=0)\n",
    "            for words in X])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "aa2adf7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray) -> Dict[int, float]:\n",
    "    \"\"\"\n",
    "    Вычисление метрики accuracy для каждого класса\n",
    "    y_true - истинные значения классов\n",
    "    y_pred - предсказанные значения классов\n",
    "    Возвращает словарь: ключ - метка класса, \n",
    "    значение - Accuracy для данного класса\n",
    "    \"\"\"\n",
    "    # Для удобства фильтрации сформируем Pandas DataFrame \n",
    "    d = {'t': y_true, 'p': y_pred}\n",
    "    df = pd.DataFrame(data=d)\n",
    "    # Метки классов\n",
    "    classes = np.unique(y_true)\n",
    "    # Результирующий словарь\n",
    "    res = dict()\n",
    "    # Перебор меток классов\n",
    "    for c in classes:\n",
    "        # отфильтруем данные, которые соответствуют \n",
    "        # текущей метке класса в истинных значениях\n",
    "        temp_data_flt = df[df['t']==c]\n",
    "        # расчет accuracy для заданной метки класса\n",
    "        temp_acc = accuracy_score(\n",
    "            temp_data_flt['t'].values, \n",
    "            temp_data_flt['p'].values)\n",
    "        # сохранение результата в словарь\n",
    "        res[c] = temp_acc\n",
    "    return res\n",
    "\n",
    "def print_accuracy_score_for_classes(\n",
    "    y_true: np.ndarray, \n",
    "    y_pred: np.ndarray):\n",
    "    \"\"\"\n",
    "    Вывод метрики accuracy для каждого класса\n",
    "    \"\"\"\n",
    "    accs = accuracy_score_for_classes(y_true, y_pred)\n",
    "    if len(accs)>0:\n",
    "        print('Метка \\t Accuracy')\n",
    "    for i in accs:\n",
    "        print('{} \\t {}'.format(i, accs[i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "7b57ba6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Обучающая и тестовая выборки\n",
    "boundary = 1500\n",
    "X_train = corpus[:boundary] \n",
    "X_test = corpus[boundary:]\n",
    "y_train = newsgroups['target'][:boundary]\n",
    "y_test = newsgroups['target'][boundary:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "0c3ffad6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:763: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Метка \t Accuracy\n",
      "0 \t 0.8640350877192983\n",
      "1 \t 0.9320388349514563\n",
      "2 \t 0.8256880733944955\n",
      "3 \t 0.7587719298245614\n"
     ]
    }
   ],
   "source": [
    "sentiment_2(EmbeddingVectorizer(model_imdb.wv), LogisticRegression(C=5.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e33c008f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88349156",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af43d24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "166c08fe",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6250ce",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30711678",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50220df9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d950d8ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b7630d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a06755",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2ae783b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
